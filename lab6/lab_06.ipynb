{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "195360da-9f12-47bf-8867-216253d85a6b",
   "metadata": {},
   "source": [
    "## Zad 1.\n",
    "Napisz własną implementację metody GMM, która będzie modelować dane za pomocą mieszaniny gaussowskiej. Wygeneruj trzy dwuwymiarowe rozkłady normalne o poniższych  parametrach:\n",
    "- $\\mu_1 = (2, 2)^T$, $\\Sigma_1 = \\begin{bmatrix}\n",
    "1 & 0.3 \\\\\n",
    "0.3 & 0.5\n",
    "\\end{bmatrix}$, $n_1 = 300$,\n",
    "- $\\mu_2 = (-1, -2)^T$, $\\Sigma_2 = \\begin{bmatrix}\n",
    "0.8 & -0.4 \\\\\n",
    "-0.4 & 0.8\n",
    "\\end{bmatrix}$, $n_2 = 200$,\n",
    "- $\\mu_3 = (3, -2)^T$, $\\Sigma_3 = \\begin{bmatrix}\n",
    "1.2 & 0 \\\\\n",
    "0 & 0.3\n",
    "\\end{bmatrix}$, $n_3 = 400$.\n",
    "\n",
    "Stwórz wykres prezentujący wygenerowane dane z oryginalnym podziałem na klasy. Dane połącz w jeden zbiór `X` o wymiarach $(900, 2)$, a następnie wytrenuj własną implementację GMM na wygenerowanych danych `X`. Dodatkowo, wytrenuj `sklearn.mixture.GaussianMixture` na tych samych danych i narysuj na jednym wykresie (używając [subplots z pakietu matplotlib](https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplots_demo.html)):\n",
    "- wykres z oryginalnym podziałem na klasy,\n",
    "- wykres z predykcjami własnej implementacji GMM,\n",
    "- wykres z predykcjami GMM z `scikit-learn`.\n",
    "\n",
    "Na każdym wykresie dodaj elipsy pokazujące dopasowane rozkłady. Oblicz [ARI (Adjusted Rand Index)](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_rand_score.html) między oryginałem, a:\n",
    "- własnym GMM,\n",
    "- scikit-learn GMM.\n",
    "\n",
    "Porównaj log-likelihood obu modeli oraz ich czasy trenowania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fc22ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from torch import nn\n",
    "from sklearn.datasets import make_blobs, make_circles, make_moons\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e88b898-1530-476c-a208-846afd25f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    def __init__(self, n_components, max_iter=100, tol=1e-6):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        \n",
    "    def _initialize_parameters(self, X):\n",
    "        # Inicjalizacja wag, średnich i kowariancji\n",
    "        # πₖ = 1/K, μₖ - losowe punkty z danych, Σₖ = macierz identycznościowa\n",
    "        pass\n",
    "        \n",
    "    def _multivariate_normal_pdf(self, X, mean, cov):\n",
    "        # Oblicz N(x | μ, Σ) dla wielu punktów\n",
    "        pass\n",
    "        \n",
    "    def _e_step(self, X):\n",
    "        # Oblicz odpowiedzialności γ(zₙₖ)\n",
    "        # Zwróć log-likelihood\n",
    "        pass\n",
    "        \n",
    "    def _m_step(self, X, responsibilities):\n",
    "        # Aktualizuj parametry: πₖ, μₖ, Σₖ\n",
    "        pass\n",
    "        \n",
    "    def fit(self, X):\n",
    "        # Główna pętla EM\n",
    "        # Inicjalizacja parametrów\n",
    "        for iter in range(self.max_iter):\n",
    "            # Kroki E i M\n",
    "            # Sprawdź zbieżność (zmiana log-likelihood < tol)\n",
    "        pass\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Przypisz punkty do składowej z najwyższą odpowiedzialnością\n",
    "        pass\n",
    "        \n",
    "    def score_samples(self, X):\n",
    "        # Oblicz log-prawdopodobieństwo dla każdej próbki\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f876280a-20c5-46e6-8709-51ec6d8ba55a",
   "metadata": {},
   "source": [
    "### Zad 2.\n",
    "Zamiast używać algorytmu EM, możemy minimalizować negatywną log-wiarygodność przy użyciu metod gradientowych. Użyj pakietu `PyTorch` do implementacji klasy `GMMTorch` wykorzystującą automatyczne różniczkowanie. Użyj optymalizatora `Adam` oraz wykorzystaj log-sum-exp trick dla stabilności numerycznej. Policz ARI (Adjusted Rand Index) oraz narysuj wykresy:\n",
    "- z oryginalnym podziałem na klasy,\n",
    "- z predykcjami implementacji GMMTorch,\n",
    "- wartosci funkcji straty podczas optymalizacji gradientowej. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb99f4b-695c-4079-bd85-d8de94cc81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMMTorch(nn.Module):\n",
    "    def __init__(self, n_components, n_features):\n",
    "        # Inicjalizacja parametrów jako nn.Parameter\n",
    "        # Użyj reprezentacji Cholesky dla macierzy kowariancji\n",
    "    \n",
    "    def get_covariance_matrices(self):\n",
    "        # Rekonstrukcja macierzy kowariancji z postaci Cholesky\n",
    "    \n",
    "    def get_softmax_weights(self):\n",
    "        # Transformacja wag do postaci spełniającej warunek sumy = 1\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Obliczenie negatywnej log-wiarygodności\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Przypisanie punktów do składowych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60530267-d488-4d3c-a280-d2cc1c712e04",
   "metadata": {},
   "source": [
    "### Zad 3.\n",
    "Celem zadania jest implementacja metody Cross-Entropy Clustering (CEC) - alternatywnego podejścia do klasteryzacji opartego na minimizacji cross-entropii między rozkładami danych a modelami probabilistycznymi, bez założenia mieszaniny probabilistycznej. Zaimplementuj klasę `CECGaussian` wykorzystującą rozkłady normalne i porównaj ją do metody GMM oraz [KMeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fce464e-3ffc-4e96-956a-6e72bf07ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CECGaussian:\n",
    "    def __init__(self, n_clusters, max_iter=100, tol=1e-6, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def _initialize_models(self, X):\n",
    "        # Inicjalizacja średnich, kowariancji i proporcji\n",
    "        \n",
    "    def _gaussian_log_prob(self, X, mean, cov):\n",
    "        # Obliczenie log-gęstości rozkładu normalnego\n",
    "        \n",
    "    def _calculate_cost(self, X, labels):\n",
    "        # Obliczenie wartości funkcji kosztu CEC\n",
    "        \n",
    "    def _assign_points(self, X):\n",
    "        # Przypisanie punktów do klastrów\n",
    "        \n",
    "    def _update_models(self, X, labels):\n",
    "        # Aktualizacja parametrów modeli\n",
    "        \n",
    "    def fit(self, X):\n",
    "        # Główna pętla algorytmu\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Predykcja etykiet dla nowych danych\n",
    "\n",
    "\n",
    "\n",
    "def generate_test_datasets():\n",
    "    \"\"\"Generate four different types of test datasets\"\"\"\n",
    "    np.random.seed(42)\n",
    "    datasets = {}\n",
    "    \n",
    "    # 1. Standard Gaussian data (ideal case for GMM)\n",
    "    X_gaussian, y_gaussian = make_blobs(\n",
    "        n_samples=900, centers=3, cluster_std=[1.0, 0.8, 1.2], \n",
    "        random_state=42, center_box=(-5, 5)\n",
    "    )\n",
    "    datasets['gaussian'] = (X_gaussian, y_gaussian)\n",
    "    \n",
    "    # 2. Data with significantly different variances\n",
    "    centers = np.array([[2, 2], [-2, -2], [0, 5]])\n",
    "    covariances = [\n",
    "        np.array([[2.0, 0.8], [0.8, 0.5]]),    # Large, correlated\n",
    "        np.array([[0.2, 0.0], [0.0, 0.2]]),    # Small, spherical\n",
    "        np.array([[0.1, -0.3], [-0.3, 1.5]])   # Medium, anti-correlated\n",
    "    ]\n",
    "    \n",
    "    X_varying = []\n",
    "    y_varying = []\n",
    "    samples_per_cluster = [300, 300, 300]\n",
    "    for i, (center, cov) in enumerate(zip(centers, covariances)):\n",
    "        data = np.random.multivariate_normal(center, cov, samples_per_cluster[i])\n",
    "        X_varying.append(data)\n",
    "        y_varying.extend([i] * samples_per_cluster[i])\n",
    "    \n",
    "    datasets['varying_var'] = (np.vstack(X_varying), np.array(y_varying))\n",
    "    \n",
    "    # 3. Data with outliers\n",
    "    X_clean, y_clean = make_blobs(n_samples=850, centers=3, cluster_std=1.0, random_state=42)\n",
    "    \n",
    "    # Add different types of outliers\n",
    "    outliers_uniform = np.random.uniform(-10, 10, (30, 2))\n",
    "    outliers_far = np.random.uniform(-15, -12, (10, 2))\n",
    "    outliers_far2 = np.random.uniform(12, 15, (10, 2))\n",
    "    \n",
    "    X_outliers = np.vstack([X_clean, outliers_uniform, outliers_far, outliers_far2])\n",
    "    y_outliers = np.concatenate([y_clean, np.full(50, -1)])  # -1 for outliers\n",
    "    \n",
    "    datasets['with_outliers'] = (X_outliers, y_outliers)\n",
    "    \n",
    "    # 4. Non-Gaussian data (structures that violate Gaussian assumption)\n",
    "    # Circles + line structure\n",
    "    X_circle1, _ = make_circles(n_samples=300, factor=0.5, noise=0.05, random_state=42)\n",
    "    X_circle2, _ = make_circles(n_samples=300, factor=0.3, noise=0.05, random_state=42)\n",
    "    X_circle2 = X_circle2 * 0.5 + np.array([2, 0])\n",
    "    \n",
    "    # Linear structure\n",
    "    t = np.linspace(-2, 2, 300)\n",
    "    X_line = np.column_stack([t, 0.5 * t + 0.1 * np.random.randn(300)]) - np.array([1, 1])\n",
    "    \n",
    "    X_non_gauss = np.vstack([X_circle1 * 3, X_circle2 * 2, X_line])\n",
    "    y_non_gauss = np.concatenate([np.zeros(300), np.ones(300), np.full(300, 2)])\n",
    "    \n",
    "    datasets['non_gaussian'] = (X_non_gauss, y_non_gauss)\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "\n",
    "datasets = generate_test_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b683e72-6c5c-40bc-aa98-ea6d427a3e05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
