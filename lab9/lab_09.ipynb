{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b50e46-0b02-4551-86b0-f66801f3c3c8",
   "metadata": {},
   "source": [
    "## Zad 1.\n",
    "Poniżej mają Państwo implementację, trening oraz generowanie z przestrzeniu latent prostego modelu `Real NVP` w PyTorch, zdolnego do nauczenia się i generowania próbek. Proszę wykonać poniższe instrukcje:\n",
    "- zaimplementuj funkcję `inverse` z klasy `AffineCoupling`, która musi odwrócić transformację z funkcji `forward`,\n",
    "- sprawdź funkcję `sigmoid` w modelu `self.net_s` zamiast funkcji `tanh`,\n",
    "- dobierz odpowienie parametry uczenia się modelu:\n",
    "    - learning rate,\n",
    "    - wpływ liczby bloków,\n",
    "    - zmień rozkład bazowy $p_z$ ze standardowego rozkładu normalnego na rozkład jednostajny i sprawdź, jak wpływa to na proces uczenia i wyniki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f77f33-a972-417a-bf06-c585529e2d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class AffineCoupling(nn.Module):\n",
    "    \"\"\"\n",
    "    Affine Coupling Layer (kluczowy element Real NVP).\n",
    "    Wykonuje transformację: y = [x1, x2 * exp(s(x1)) + t(x1)]\n",
    "    gdzie x1 to połowa wektora wejściowego, a s i t to skalar i translacja.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features, mask):\n",
    "        super().__init__()\n",
    "        self.mask = mask\n",
    "        \n",
    "        # Sieci neuronowe s (skala) i t (translacja), które biorą maskowaną część wejścia\n",
    "        # i generują parametry dla drugiej części.\n",
    "        self.net_s = nn.Sequential(\n",
    "            nn.Linear(in_features // 2, hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_features, hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_features, in_features // 2),\n",
    "            nn.Tanh()     # <---  sprawdzić sigmoidę\n",
    "        )\n",
    "        \n",
    "        self.net_t = nn.Sequential(\n",
    "            nn.Linear(in_features // 2, hidden_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_features, in_features // 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\" Transformacja do przodu (z x do z), używana do treningu. \"\"\"\n",
    "        # Zmienna log_det_jacobian będzie sumą log|det(J)| dla każdego bloku.\n",
    "        log_det_jacobian = 0.0\n",
    "        \n",
    "        x1 = (x @ self.mask).view(x.shape[0], -1)  # Nie transformowana część\n",
    "        x2 = (x @ (1 - self.mask)).view(x.shape[0], -1) # Transformowana część\n",
    "        \n",
    "        s = self.net_s(x1)\n",
    "        t = self.net_t(x1)\n",
    "        \n",
    "        z2_transformed = x2 * torch.exp(s) + t\n",
    "\n",
    "        temp = (torch.arange(self.mask.numel(), device=x.device, dtype=x.dtype).tile(x.shape[0], 1) @ self.mask).view(x.shape[0], -1).long()\n",
    "        \n",
    "        z = torch.zeros_like(x)\n",
    "        z.scatter_(1, temp, x1)\n",
    "\n",
    "        temp = (torch.arange(self.mask.numel(), device=x.device, dtype=x.dtype).tile(x.shape[0], 1) @ (1 - self.mask)).view(x.shape[0], -1).long()\n",
    "        z.scatter_(1, temp, z2_transformed)\n",
    "        \n",
    "        # log|det(J)| = suma log(exp(s)) = suma s\n",
    "        # Ponieważ transformacja ma formę y = [x1, exp(s)*x2 + t], jakobian jest macierzą trójkątną.\n",
    "        # Determinanta to iloczyn elementów na przekątnej, a logarytm z determinanty to suma logarytmów.\n",
    "        log_det_jacobian += torch.sum(s, dim=1)\n",
    "        \n",
    "        return z, log_det_jacobian\n",
    "\n",
    "    def inverse(self, z):\n",
    "        \"\"\" Transformacja odwrotna (z z do x), używana do generowania próbek. \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class RealNVP(nn.Module):\n",
    "    \"\"\"\n",
    "    Główny model Normalizing Flow, składający się z sekwencji bloków Affine Coupling.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features, num_blocks):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Rozkład bazowy (prior), z którego losujemy próbki w przestrzeni 'z'.\n",
    "        # Zwykle używa się standardowego rozkładu normalnego (Gaussowski).\n",
    "        self.base_dist = MultivariateNormal(\n",
    "            loc=torch.zeros(in_features).to(device),\n",
    "            covariance_matrix=torch.eye(in_features).to(device)\n",
    "        )\n",
    "        \n",
    "        self.flows = nn.ModuleList()\n",
    "        # Tworzenie sekwencji bloków Affine Coupling\n",
    "        for i in range(num_blocks):\n",
    "            # Maska naprzemienna, aby każda cecha miała szansę być transformowana.\n",
    "            # np. dla dim=2, maska to [1, 0] lub [0, 1]\n",
    "            # [1, 0]: transformuje drugą cechę bazując na pierwszej.\n",
    "            # [0, 1]: transformuje pierwszą cechę bazując na drugiej.\n",
    "            mask = torch.zeros(in_features)\n",
    "            mask[i % in_features::2] = 1.0 # Naprzemienne maskowanie co 2-gi blok\n",
    "            mask = mask.to(device)\n",
    "            \n",
    "            self.flows.append(\n",
    "                AffineCoupling(in_features, hidden_features, mask)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Trening: Przekształcenie danych wejściowych x do przestrzeni bazowej z\n",
    "        i obliczenie log-wiarygodności (log-likelihood).\n",
    "        \"\"\"\n",
    "        log_prob = 0.0\n",
    "        z = x\n",
    "        \n",
    "        # Iteracja przez wszystkie bloki transformacji (Flows)\n",
    "        for flow in self.flows:\n",
    "            z, log_det_jacobian = flow(z)\n",
    "            log_prob += log_det_jacobian\n",
    "            \n",
    "        # P(x) = P(z) * |det(dz/dx)|^-1  -> log P(x) = log P(z) - log|det(dz/dx)|\n",
    "        # log|det(dx/dz)| = -log|det(dz/dx)|\n",
    "        # W Real NVP transformacja jest z x do z: dx/dz = (dz/dx)^-1\n",
    "        # log|det(dz/dx)| = log_det_jacobian (zwracany przez flow)\n",
    "        # Ostateczna log-wiarygodność:\n",
    "        log_prob += self.base_dist.log_prob(z)\n",
    "        \n",
    "        return log_prob.mean(), z\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        \"\"\"\n",
    "        Generowanie próbek: Losowanie z z i przekształcenie z do przestrzeni danych x.\n",
    "        \"\"\"\n",
    "        # Losowanie z rozkładu bazowego (z)\n",
    "        z = self.base_dist.sample((num_samples,))\n",
    "        \n",
    "        # Iteracja przez bloki transformacji w odwrotnej kolejności\n",
    "        x = z\n",
    "        for flow in reversed(self.flows):\n",
    "            x = flow.inverse(x)\n",
    "            \n",
    "        return x\n",
    "\n",
    "\n",
    "def generate_data(name, num_samples=1000):\n",
    "    \"\"\"\n",
    "    Generuje dane spiralne lub pierścieniowe.\n",
    "    \"\"\"\n",
    "    if name == 'spiral':\n",
    "        t = np.linspace(0, 4 * np.pi, num_samples)\n",
    "        x = t * np.cos(t) + np.random.normal(0, 0.2, num_samples)\n",
    "        y = t * np.sin(t) + np.random.normal(0, 0.2, num_samples)\n",
    "        data = np.stack([x, y], axis=1)\n",
    "    elif name == 'ring':\n",
    "        t = np.random.uniform(0, 2 * np.pi, num_samples)\n",
    "        r = 10 + np.random.normal(0, 0.5, num_samples)\n",
    "        x = r * np.cos(t)\n",
    "        y = r * np.sin(t)\n",
    "        data = np.stack([x, y], axis=1)\n",
    "    else:\n",
    "        raise ValueError(\"Nieznany typ danych.\")\n",
    "\n",
    "    # Konwersja do tensora PyTorch\n",
    "    data = torch.tensor(data, dtype=torch.float32).to(device)\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def train_and_visualize(data_type='spiral', num_blocks=4, num_epochs=5000):\n",
    "    \"\"\"\n",
    "    Funkcja trenująca i wizualizująca wyniki dla danego typu danych.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Trening dla danych: {data_type.upper()} ---\")\n",
    "    data = generate_data(data_type)\n",
    "    \n",
    "    # Inicjalizacja modelu\n",
    "    in_features = 2\n",
    "    hidden_features = 64\n",
    "    model = RealNVP(in_features, hidden_features, num_blocks).to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Pętla treningowa\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Obliczenie straty (maksymalizacja log-wiarygodności)\n",
    "        # PyTorch domyślnie minimalizuje straty, więc minimalizujemy ujemną log-wiarygodność.\n",
    "        log_likelihood, _ = model(data)\n",
    "        loss = -log_likelihood \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 500 == 0:\n",
    "            print(f'Epoka {epoch}/{num_epochs}, Ujemna Log-Wiarygodność (Loss): {loss.item():.4f}')\n",
    "\n",
    "    # Wizualizacja wyników\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Generowanie nowych próbek\n",
    "        samples = model.sample(1000).cpu().numpy()\n",
    "        \n",
    "        # Wizualizacja transformacji x -> z\n",
    "        _, z_transformed = model(data)\n",
    "        z_transformed = z_transformed.cpu().numpy()\n",
    "        \n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Oryginalne dane\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.scatter(data.cpu().numpy()[:, 0], data.cpu().numpy()[:, 1], s=10)\n",
    "    plt.title(f'1. Oryginalne dane ({data_type.capitalize()})')\n",
    "    plt.xlabel('x1'); plt.ylabel('x2')\n",
    "    plt.grid(True, linestyle='--')\n",
    "    \n",
    "    # Przestrzeń latent (z)\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.scatter(z_transformed[:, 0], z_transformed[:, 1], s=10)\n",
    "    plt.title('2. Transformacja do przestrzeni bazowej (z)')\n",
    "    plt.xlabel('z1'); plt.ylabel('z2')\n",
    "    plt.xlim(-5, 5); plt.ylim(-5, 5)\n",
    "    plt.grid(True, linestyle='--')\n",
    "\n",
    "    # Wygenerowane próbki\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.scatter(samples[:, 0], samples[:, 1], s=10)\n",
    "    plt.title('3. Wygenerowane próbki (Flow)')\n",
    "    plt.xlabel('x1'); plt.ylabel('x2')\n",
    "    plt.grid(True, linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "# Uruchomienie treningu dla danych pierścieniowych\n",
    "train_and_visualize(data_type='ring', num_blocks=8, num_epochs=10000)\n",
    "\n",
    "# Uruchomienie treningu dla danych spiralnych\n",
    "# train_and_visualize(data_type='spiral', num_blocks=8, num_epochs=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fd7e3a-1719-4c21-9866-f71e4465b2ec",
   "metadata": {},
   "source": [
    "## Zad 2.\n",
    "Korzystając z kodów z [repozytorium GitHub](https://github.com/GSavathrakis/Glow-pytorch/tree/main), naucz model `GLOW` na zbiorze mnist. Po nauczeniu modelu wygeneruj kilka obrazków z przestrzeni latent. Użyj funkcji [t-sne](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) do wizualizacji przestrzeni latentej na podstawie punktów reprezentujących dane (obrazy), stosując różne kolory dla poszczególnych klas.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494d83d8-d61a-494a-8376-6c97de105775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cb613a8-328b-44d9-9436-66b29f2e74da",
   "metadata": {},
   "source": [
    "## Zad 3.\n",
    "Przeanalizuj metodę flow-matching na podstawie podanego [jupyter notebooka](https://github.com/rfangit/analytical_flow_matching/blob/main/2D%20Examples/Linear%20Schedule%20-%20Other%20Distributions/Linear%20Schedule%20-%20Checkerboard.ipynb). Powtórz eksperymenty w nim zawarte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a580b9b3-ffee-4983-a64a-d8672e196a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
